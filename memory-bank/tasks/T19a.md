# T19a: LLM-Enhanced Search Query Parsing
*Created: 2025-05-14*
*Last Updated: 2025-05-14*

**Description**: Implement intelligent search query parsing using LLM to improve search accuracy and user experience by automatically identifying and categorizing different components of the search query (authors, topics, years, institutions, etc.).
**Status**: ðŸ”„ In Progress
**Priority**: HIGH
**Started**: 2025-05-14
**Last Active**: 2025-05-14
**Dependencies**: T19 (Historical Paper Search Enhancement), T1 (Custom arXiv API Integration)

## Completion Criteria
- Implement LLM-based query parser that accurately identifies:
  - Author names
  - Scientific topics/concepts
  - Years/date ranges
  - Institutions
  - Journal references
- Convert parsed fields into proper arXiv API search syntax
- Add error handling and fallback mechanisms
- Ensure response time stays under 100ms
- Add caching for common queries

## Implementation Plan

### Phase 1: Core Parser Implementation
1. Set up TinyLlama 1.1B with Cloudflare Workers AI
2. Create QueryParser class structure
3. Implement LLM integration
4. Add conversion to arXiv API format
5. Set up basic error handling

### Phase 2: ArXiv Integration
1. Integrate with existing search functionality
2. Add caching layer
3. Implement fallback mechanisms
4. Add performance monitoring

### Phase 3: Refinement
1. Optimize response times
2. Add edge case handling
3. Improve error messages
4. Add usage analytics

## Technical Solution

### Selected LLM: TinyLlama 1.1B
- **Model Size**: 1.1B parameters (extremely lightweight)
- **Speed**: Very low latency (40-60ms range)
- **Deployment**: Cloudflare Workers AI
- **Cost**: Free tier includes 100,000 requests/day
- **Benefits**: 
  - No infrastructure to manage
  - Global edge deployment for low latency
  - Highly suitable for real-time parsing
  - Free for early-stage development

### Implementation with Cloudflare Workers
```javascript
export default {
  async fetch(request, env) {
    // Only allow POST requests
    if (request.method !== 'POST') {
      return new Response('Method not allowed', { status: 405 });
    }
    
    try {
      // Parse the request body
      const requestData = await request.json();
      const searchQuery = requestData.query || '';
      
      // Prepare the prompt for TinyLlama
      const messages = [
        {
          role: 'system',
          content: 'You are a search parsing assistant. Extract structured data from search queries. Format results as JSON with fields like: intent, location, filters, sort_by, etc. Only include fields that are relevant to the query. Be concise.'
        },
        {
          role: 'user',
          content: `Parse this search query: "${searchQuery}"`
        }
      ];
      
      // Call Cloudflare AI API with TinyLlama
      const aiResponse = await fetch('https://api.cloudflare.com/client/v4/accounts/' + env.ACCOUNT_ID + '/ai/run/@cf/tinyllama/tinyllama-1.1b-chat-v1.0', {
        method: 'POST',
        headers: {
          'Authorization': 'Bearer ' + env.API_TOKEN,
          'Content-Type': 'application/json'
        },
        body: JSON.stringify({ messages })
      });
      
      const aiData = await aiResponse.json();
      
      // Return the parsed result
      return new Response(JSON.stringify({
        original_query: searchQuery,
        parsed_result: aiData.result.response
      }), {
        headers: { 'Content-Type': 'application/json' }
      });
    } catch (error) {
      return new Response(JSON.stringify({ error: error.message }), {
        status: 500,
        headers: { 'Content-Type': 'application/json' }
      });
    }
  }
};
```

## Related Files
- `src/lib/queryParser.ts` (new)
- `src/lib/arxiv.ts`
- `src/types/query.ts` (new)
- `src/hooks/useArxiv.ts`
- `tests/lib/queryParser.test.ts` (new)
- `workers/search-parser.js` (new - Cloudflare Worker)

## Technical Considerations
1. **LLM Integration**:
   - Using TinyLlama 1.1B via Cloudflare Workers AI
   - Free tier includes 100,000 requests/day
   - Low latency (40-60ms typical response time)
   - No rate limiting concerns for early development

2. **Performance**:
   - Implement client-side caching for repeat queries
   - Add timeout handling (30s max for Workers)
   - Monitor response times through Cloudflare dashboard
   - Use edge deployment for global low latency

3. **Error Handling**:
   - Graceful fallback to basic search
   - Clear error messages
   - Retry mechanism for API failures
   - Logging for improvements

## Progress
1. âœ… Research and select appropriate LLM solution
2. âœ… Create QueryParser class structure
3. âœ… Implement LLM integration using Gemini 1.5 Flash
4. âœ… Add conversion to arXiv format
5. âœ… Integrate with existing search
6. âœ… Implement caching layer
7. âœ… Add error handling and type safety
8. âœ… Performance optimization (using Flash model variant)
9. âœ… Improve search UX (trigger on Enter/button only)
10. ðŸ”„ Documentation and testing

### Session Notes (2025-05-15)
1. Implemented and tested LLM query parsing with Gemini 1.5 Flash
   - Successfully parses authors, topics, year ranges, categories
   - Very efficient: ~50-70 tokens per request
   - High confidence outputs (avgLogprobs near 0)
   - Clean JSON output format

2. Enhanced QueryParser Implementation
   - Added proper TypeScript interfaces
   - Improved error handling and fallbacks
   - Added smart author name handling
   - Proper capitalization preservation
   - Added relaxed search fallback for no results

3. Search UX Improvements
   - Modified to trigger only on Enter/button click
   - Added local state management
   - Maintained history functionality
   - Better error handling

4. Fixed Various Issues
   - TypeScript type safety improvements
   - Added missing Paper interface fields
   - Fixed build errors
   - Added null checks and optional fields

5. Pagination Implementation (2025-05-15 Evening)
   - Added support for 20/50/100 results per page
   - Implemented proper metadata extraction from ArXiv API
   - Added pagination controls with First/Previous/Next/Last navigation
   - Fixed metadata handling in API responses
   - Added proper TypeScript types for pagination
   - Integrated with existing search state management

### Next Steps
1. Complete documentation
2. Add comprehensive tests
3. Monitor performance metrics
4. Consider additional error handling cases

## Future Enhancement: Automated Prompt Optimization System

### Problem Statement
Current approach of manually fine-tuning LLM prompts for edge cases (names, phrases, pronouns, etc.) is not sustainable and leads to constant prompt adjustments for various query combinations.

### Potential Systematic Solutions

#### Option 1: Result Quality Heuristics (No Manual Labels)
- **Validation Rules**:
  - Query should return >0 results if it contains valid scientific terms
  - Results should be reasonably recent (not all from 1990s)
  - Query parsing should be consistent across similar inputs
  - More specific queries should return fewer results than broader ones
- **Implementation**: Automated testing framework that flags obviously broken parsing

#### Option 2: Self-Validation Using Multiple Parsing Approaches
- Parse same query with different prompts/approaches
- Consensus-based validation (agreement = likely correct, disagreement = use fallback)
- No manual labeling required

#### Option 3: Incremental Learning from User Behavior
- Track user interaction patterns (clicks, abandons, result engagement)
- Low interaction rate indicates poor parsing quality
- Build validation dataset from actual usage patterns over time

#### Option 4: Comparative Validation
- Compare LLM parsing results against simple keyword search
- Flag cases where LLM returns zero results but keyword search returns many
- Use result count similarity as validation metric

### Automated Testing Framework Design
```
1. Query Generation:
   - Random combinations of authors, topics, years, categories
   - Real-world pattern sampling
   - Systematic edge case testing (abbreviations, multi-word terms, ambiguous names)

2. Validation Pipeline:
   - Run queries through current parser
   - Apply result quality heuristics
   - Flag failures and patterns
   - Generate prompt variations targeting failures

3. Optimization Loop:
   - A/B test prompt variations
   - Measure success metrics (parsing accuracy, result relevance, consistency)
   - Iteratively improve based on failure patterns
```

### Implementation Priority
- **Phase 1**: Implement basic result quality heuristics validation
- **Phase 2**: Add comparative validation against keyword search
- **Phase 3**: Build automated prompt optimization system
- **Phase 4**: Consider user behavior tracking for long-term improvement

### Technical Considerations
- Start with simple rules like "parsing that returns 0 results for common scientific terms is probably wrong"
- Build framework incrementally without requiring manual labeling
- Focus on detecting obviously broken parsing rather than perfect accuracy initially